{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e7447-83db-410f-adaa-c61cbd540ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "import argparse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff3b74da-f46e-49c5-abce-7d0223a4d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af12a60-67b0-4b9d-ab82-efbaeb1640f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"secrets/token.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "HUGGINGFACE_HUB_TOKEN = data[\"HUGGINGFACE_HUB_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb5f224-3c71-4f54-871a-a6f2531aafea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Monkey-patch get_default_device for this session\n",
    "if not hasattr(torch, \"get_default_device\"):\n",
    "    def _get_default_device():\n",
    "        return torch.device(\"mps\") if (torch.backends.mps.is_built() and torch.backends.mps.is_available()) else torch.device(\"cpu\")\n",
    "    torch.get_default_device = _get_default_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1243f-8a54-4b48-bab5-92349ff417ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \n",
    "    category=UserWarning, \n",
    "    module=\"transformers.pytorch_utils\"\n",
    ")\n",
    "\n",
    "# Logging helper\n",
    "def log(msg: str):\n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}\")\n",
    "\n",
    "# Device detection (will now agree with our monkey‐patch)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_built() and torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "    \n",
    "# Model and tokenizer setup\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "log(f\"Loading tokenizer for {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    token=HUGGINGFACE_HUB_TOKEN,\n",
    ")\n",
    "log(\"Tokenizer loaded.\")\n",
    "\n",
    "log(f\"Loading model {MODEL_ID} (float16, auto device_map)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    token=HUGGINGFACE_HUB_TOKEN,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=None,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "# Move model to MPS (Apple GPU) if available, else CPU\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "log(f\"Model loaded and moved to {device}.\")\n",
    "\n",
    "# Default generation parameters\n",
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"do_sample\": True,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "# Static system prompt\n",
    "date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "SYSTEM_PROMPT = f\"\"\"\n",
    "You are an expert macOS system administrator that classifies file paths as safe-to-delete cache files or not.\n",
    "Respond in JSON only.\n",
    "Current date: {date}\n",
    "\"\"\"\n",
    "\n",
    "# Build user prompt\n",
    "def get_user_prompt(n: int) -> str:\n",
    "    return f\"\"\"\n",
    "    Generate a `multi_language_dev_dataset`: a JSON array of at least {n} objects covering common dev-cache/build folders on macOS across various ecosystems:\n",
    "          - Python (`__pycache__`, `venv`)\n",
    "          - Node.js (`~/.npm`, `node_modules`)\n",
    "          - Java/Maven (`~/.m2/repository`, `target`)\n",
    "          - Android/Gradle (`.gradle`, `build/`)\n",
    "          - Go (`~/go/pkg`, `~/go/bin`)\n",
    "          - Rust/Cargo (`target/`)\n",
    "          - Docker (`~/.docker`)\n",
    "          - VSCode (`~/.vscode/extensions`)\n",
    "          - .NET/NuGet (`~/.nuget/packages`)\n",
    "          - Ruby/Bundler (`~/.bundle`, `~/.gem`)\n",
    "        For each entry pick realistic:\n",
    "          • `file_name` & `file_path`\n",
    "          • `extension` (or empty string)\n",
    "          • `size_mb`\n",
    "          • `last_accessed_days`\n",
    "          • `category`\n",
    "          • `tool`\n",
    "          • `label`\n",
    "          • `reason`\n",
    "        Respond only with the JSON array.\n",
    "    \"\"\"\n",
    "\n",
    "# Tokenization helper\n",
    "def build_input(system: str, user: str):\n",
    "    enc_sys = tokenizer(system, return_tensors=\"pt\")\n",
    "    enc_usr = tokenizer(user, return_tensors=\"pt\")\n",
    "    input_ids = torch.cat([enc_sys.input_ids, enc_usr.input_ids], dim=1).to(device)\n",
    "    attention_mask = torch.cat([enc_sys.attention_mask, enc_usr.attention_mask], dim=1).to(device)\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "# Generate and save raw + parse JSON\n",
    "def generate_dataset(n_examples: int = 50, raw_dir: Path = Path(\"raw_txt\"), json_dir: Path = Path(\"output_json\")):\n",
    "    raw_dir.mkdir(exist_ok=True)\n",
    "    user_prompt = get_user_prompt(n_examples)\n",
    "    ids, mask = build_input(SYSTEM_PROMPT, user_prompt)\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "    log(f\"Batch time: {time.perf_counter()-start}\")\n",
    "    \n",
    "    # Extract newly generated tokens\n",
    "    gen_tokens = out[0, ids.shape[1]:]\n",
    "    raw_text = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
    "    # Save raw output with error handling\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    uid = ''.join(random.choices(string.ascii_uppercase + string.digits, k=6))\n",
    "    raw_file = raw_dir / f\"raw_{ts}_{uid}.txt\"\n",
    "    try:\n",
    "        raw_file.write_text(raw_text, encoding=\"utf-8\")\n",
    "        log(f\"Raw output written to {raw_file}\")\n",
    "    except Exception as e:\n",
    "        log(f\"ERROR writing raw output: {e}\")\n",
    "\n",
    "    # Extract all JSON array substrings (non-greedy)\n",
    "    matches = re.findall(r\"\\[.*?\\]\", raw_text, flags=re.DOTALL)\n",
    "    log(raw_text[:50])\n",
    "    if not matches:\n",
    "        log(f\"Failed to extract any JSON array from raw output, see {raw_file}\")\n",
    "        return raw_text\n",
    "    else:\n",
    "        data = []\n",
    "        for idx, json_str in enumerate(matches, start=1):\n",
    "            try:\n",
    "                obj = json.loads(json_str)\n",
    "                log(f\"Parsed JSON array #{idx} with {len(obj)} items.\")\n",
    "                data.extend(obj if isinstance(obj, list) else [obj])\n",
    "            except json.JSONDecodeError as e:\n",
    "                log(f\"ERROR parsing JSON array #{idx}: {e}\")\n",
    "    \n",
    "        # 3) write parsed data out to JSON\n",
    "        json_dir.mkdir(parents=True, exist_ok=True)\n",
    "        json_file = json_dir / f\"output_{ts}_{uid}.json\"\n",
    "        try:\n",
    "            # indent=2 for readability; you can omit if you want compact\n",
    "            json_file.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "            log(f\"Parsed JSON written to {json_file}\")\n",
    "        except Exception as e:\n",
    "            log(f\"ERROR writing parsed JSON: {e}\")\n",
    "       # return raw_text\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea54074b-d6a8-4e94-b4df-190ba36796f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Generate multi-language dev dataset.\")\n",
    "parser.add_argument(\"-n\", \"--num\",           type=int,   default=50000, help=\"Total entries to generate.\")\n",
    "parser.add_argument(\"--chunk_size\",          type=int,   default=12,    help=\"Entries per batch prompt.\")\n",
    "parser.add_argument(\"--temperature\",         type=float, default=1.0,   help=\"Sampling temperature.\")\n",
    "parser.add_argument(\"--max_new_tokens\",      type=int,   default=2400,  help=\"Max tokens per batch\")\n",
    "parser.add_argument(\"--do_sample\",           action=\"store_true\",    help=\"Enable sampling.\")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "# Update generation kwargs from CLI\n",
    "gen_kwargs.update({\n",
    "    \"max_new_tokens\": args.max_new_tokens,\n",
    "    \"do_sample\":      args.do_sample,\n",
    "    \"temperature\":    args.temperature if args.do_sample else 1.0,\n",
    "    \"top_p\":          0.95 if args.do_sample else 1.0,\n",
    "})\n",
    "\n",
    "total = args.num\n",
    "chunk = args.chunk_size\n",
    "all_data = []\n",
    "log(f\"Generating {total} entries in chunks of {chunk}...\")\n",
    "for start in range(0, total, chunk):\n",
    "    batch = min(chunk, total - start)\n",
    "    log(f\"Batch {start//chunk+1}: generating {batch} entries...\")\n",
    "    data = generate_dataset(batch)       # uses updated gen_kwargs\n",
    "    all_data.extend(data)\n",
    "\n",
    "\n",
    "# Save JSON dataset inside parsed_json/\n",
    "parsed_dir = Path(\"parsed_json\")\n",
    "parsed_dir.mkdir(exist_ok=True)\n",
    "out_json = parsed_dir / f\"multi_language_dev_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "try:\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        #json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
    "        try:\n",
    "            json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
    "        except TypeError as e:\n",
    "            log(f\"Serialization error: {e}\")\n",
    "    log(f\"Parsed JSON written to {out_json}\")\n",
    "except Exception as e:\n",
    "    log(f\"ERROR writing JSON: {e}\")\n",
    "    \n",
    "# Build DataFrame and save CSV\n",
    "try:\n",
    "    df = pd.DataFrame(all_data)\n",
    "    cols = [\"file_name\", \"file_path\", \"extension\", \"size_mb\", \"last_accessed_days\", \"category\" ,\"tool\", \"label\", \"reason\"]\n",
    "    df = df[[c for c in cols if c in df.columns]]\n",
    "    csv_file = out_json.with_suffix('.csv')\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    log(f\"DataFrame CSV written to {csv_file}\")\n",
    "except Exception as e:\n",
    "    log(f\"ERROR creating/saving DataFrame: {e}\")\n",
    "# Print preview\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a665e-ffd7-4456-a845-2db79855e74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51cc61-915a-4561-8e6c-01c3828a4912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
