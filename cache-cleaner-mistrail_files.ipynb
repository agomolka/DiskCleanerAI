{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524e7447-83db-410f-adaa-c61cbd540ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mistral/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "import argparse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3b74da-f46e-49c5-abce-7d0223a4d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af12a60-67b0-4b9d-ab82-efbaeb1640f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"secrets/token.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "HUGGINGFACE_HUB_TOKEN = data[\"HUGGINGFACE_HUB_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb5f224-3c71-4f54-871a-a6f2531aafea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Monkey-patch get_default_device for this session\n",
    "if not hasattr(torch, \"get_default_device\"):\n",
    "    def _get_default_device():\n",
    "        return torch.device(\"mps\") if (torch.backends.mps.is_built() and torch.backends.mps.is_available()) else torch.device(\"cpu\")\n",
    "    torch.get_default_device = _get_default_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "708392ba-3152-4d5a-afec-a1c34c73d44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "[2025-06-04 16:24:47] Loading tokenizer for mistralai/Mistral-7B-Instruct-v0.3...\n",
      "[2025-06-04 16:24:48] Tokenizer loaded.\n",
      "[2025-06-04 16:24:48] Loading model mistralai/Mistral-7B-Instruct-v0.3 (float16, auto device_map)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████| 3/3 [00:12<00:00,  4.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-04 16:25:13] Model loaded and moved to mps.\n"
     ]
    }
   ],
   "source": [
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \n",
    "    category=UserWarning, \n",
    "    module=\"transformers.pytorch_utils\"\n",
    ")\n",
    "\n",
    "# Logging helper\n",
    "def log(msg: str):\n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}\")\n",
    "\n",
    "# Device detection (will now agree with our monkey‐patch)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_built() and torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "    \n",
    "# Model and tokenizer setup\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "log(f\"Loading tokenizer for {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    token=HUGGINGFACE_HUB_TOKEN,\n",
    ")\n",
    "log(\"Tokenizer loaded.\")\n",
    "\n",
    "log(f\"Loading model {MODEL_ID} (float16, auto device_map)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    token=HUGGINGFACE_HUB_TOKEN,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=None,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "# Move model to MPS (Apple GPU) if available, else CPU\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "log(f\"Model loaded and moved to {device}.\")\n",
    "\n",
    "# Default generation parameters\n",
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"do_sample\": True,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "# Static system prompt\n",
    "date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "SYSTEM_PROMPT = f\"\"\"\n",
    "You are an expert macOS system administrator that classifies file paths as safe-to-delete cache files or not.\n",
    "Respond in JSON only.\n",
    "Current date: {date}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "165243b3-a138-4338-8140-be3603e0b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt(n: int):\n",
    "    return f\"\"\"\n",
    "    You are an expert macOS system administrator.  Generate a `multi_language_dev_dataset`: a JSON array of at least {n} objects covering common (and some less‐common) dev‐cache/build folders on macOS across various ecosystems:\n",
    "    - Python (`__pycache__`, `venv`)\n",
    "    - Node.js (`~/.npm`, `node_modules`)\n",
    "    - Java/Maven (`~/.m2/repository`, `target`)\n",
    "    - Android/Gradle (`.gradle`, `build/`)\n",
    "    - Go (`~/go/pkg`, `~/go/bin`)\n",
    "    - Rust/Cargo (`target/`)\n",
    "    - Docker (`~/.docker`)\n",
    "    - VSCode (`~/.vscode/extensions`)\n",
    "      - .NET/NuGet (`~/.nuget/packages`)\n",
    "      - Ruby/Bundler (`~/.bundle`, `~/.gem`)\n",
    "      - macOS system caches (e.g. `/Library/Caches`, `~/Library/Logs`, `/private/var/folders`)\n",
    "      - videos and photos\n",
    "      - Other less‐common cache/build folders (e.g. CocoaPods caches `~/Library/Caches/CocoaPods`, Carthage caches `~/Library/Caches/org.carthage.CarthageKit`, etc.)\n",
    "    \n",
    "    For each entry, output exactly one JSON object with these keys (use the names in quotes, exactly as shown), providing realistic macOS metadata and a correct delete/keep label. Respond **only** with a JSON array—no narrative or extra fields:\n",
    "    \n",
    "    - \"item_name\": string — base name of the item (for a file, e.g. \"main.cpython-38.pyc\"; for a directory, e.g. \"__pycache__\").\n",
    "    - \"item_path\": string — full absolute path (e.g. \"/Users/alice/Library/Caches/com.example.app/cache.db\" or \"/Users/alice/Documents/project/node_modules\").\n",
    "    - \"extension\": string — file extension including the leading \".\" (e.g. \".pyc\", \".jar\"). If the item is a directory, use \"<DIR>\".\n",
    "    - \"size_mb\": number — size in megabytes (float, two decimals). For files, its own size; for directories, total size of immediate children.\n",
    "    - \"last_accessed_days\": integer — days since last opened or read (0 = today, 1 = yesterday).\n",
    "    - \"modified_days_ago\": integer — days since last modified.\n",
    "    - \"inode\": integer — inode number from `os.stat`.\n",
    "    - \"nlink\": integer — hard-link count from `os.stat`.\n",
    "    - \"is_symlink\": boolean — true if a symbolic link; false otherwise.\n",
    "    - \"mode\": string — permission bits in octal form (e.g. \"0o755\").\n",
    "    - \"owner_uid\": integer — user ID of the owner.\n",
    "    - \"owner_gid\": integer — group ID of the owner.\n",
    "    - \"kind\": string — human-readable Kind from `mdls -name kMDItemKind` (e.g. \"Folder\", \"JPEG image\", \"Unix executable\").\n",
    "    - \"uti\": string — Uniform Type Identifier from `mdls -name kMDItemContentType` (e.g. \"public.folder\", \"public.sqlite\").\n",
    "    - \"has_quarantine_flag\": boolean — true if \"com.apple.quarantine\" appears in xattr; false otherwise.\n",
    "    - \"num_xattrs\": integer — count of all extended attributes from `xattr -l`.\n",
    "    - \"specific_xattrs\": array of strings — list of attribute names (e.g. [\"com.apple.FinderInfo\", \"com.apple.ResourceFork\"]).\n",
    "    - \"is_network_volume\": boolean — true if `diskutil info` shows a network-mounted volume; false otherwise.\n",
    "    - \"is_read_only_volume\": boolean — true if that volume is marked read-only; false otherwise.\n",
    "    - \"volume_fs\": string — filesystem type (e.g. \"APFS\", \"HFS+J\") from `diskutil info`.\n",
    "    - \"dir_size_mb\": number — for directories, size in megabytes of immediate children (float, two decimals); for files, 0.00.\n",
    "    - \"total_file_count\": integer — total number of regular files under this directory (recursively); for files, 0.\n",
    "    - \"total_subdir_count\": integer — total number of nested subdirectories under this directory (recursively); for files, 0.\n",
    "    - \"extension_counts\": object — mapping of file extension to count under this directory (e.g.: \".js\": 120, \".json\": 30); for files, {{}}.\n",
    "    - \"contains_source_files\": boolean — true if any source file extension (e.g. \".java\", \".swift\", \".c\", \".cpp\", \".py\") exists under this directory; for files, false.\n",
    "    - \"contains_source_control_dir\": boolean — true if a \".git\" or \".svn\" folder is found under this directory; for files, false.\n",
    "    - \"contains_exec_binaries\": boolean — true if any executable binary (e.g. \".dylib\", \".so\", \".bin\") is found under this directory; for files, false.\n",
    "    - \"presence_of_build_specific_markers\": array of strings — list of build-specific folder names (e.g. \"target\", \"build\", \"CocoaPods\", \"Carthage\") found under this directory; for files, [].\n",
    "    - \"immutable_hidden_flags\": array of strings — contains any of [\"immutable\", \"hidden\"] if the item has a BSD immutable flag or is marked hidden; for items without either, use [].\n",
    "    - \"is_directory\": boolean — true if `os.path.isdir(path)` is true; false if a file.\n",
    "    - \"label\": integer — 1 = safe to delete; 0 = must be kept.\n",
    "    - \"reason\": string — one-sentence explanation why it’s safe (e.g. \"Stale cache folder, rarely used\") or why it must be kept (e.g. \"Contains project source files\").\n",
    "    \n",
    "    Example input paths (generate one object per path):\n",
    "    ```\n",
    "    \n",
    "    /Users/alice/Library/Caches/com.example.app/cache.db\n",
    "    /Users/alice/Documents/project/node_modules\n",
    "    /usr/local/bin/some_executable\n",
    "    /Users/alice/Library/Preferences/com.apple.TextEdit.plist\n",
    "    /Users/alice/.npm/_cacache/tmp_html\n",
    "    /Users/alice/Documents/project/build\n",
    "    /Applications/VisualStudioCode.app/Contents/Resources\n",
    "    \n",
    "    ```\n",
    "    \n",
    "    Respond only with the JSON array of objects as specified.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2e1243f-8a54-4b48-bab5-92349ff417ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization helper\n",
    "def build_input(system: str, user: str):\n",
    "    enc_sys = tokenizer(system, return_tensors=\"pt\")\n",
    "    enc_usr = tokenizer(user, return_tensors=\"pt\")\n",
    "    input_ids = torch.cat([enc_sys.input_ids, enc_usr.input_ids], dim=1).to(device)\n",
    "    attention_mask = torch.cat([enc_sys.attention_mask, enc_usr.attention_mask], dim=1).to(device)\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "# Generate and save raw + parse JSON\n",
    "def generate_dataset(n_examples: int = 50, raw_dir: Path = Path(\"raw_txt\"), json_dir: Path = Path(\"output_json\")):\n",
    "    raw_dir.mkdir(exist_ok=True)\n",
    "    user_prompt = get_user_prompt(n_examples)\n",
    "    ids, mask = build_input(SYSTEM_PROMPT, user_prompt)\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "    log(f\"Batch time: {time.perf_counter()-start}\")\n",
    "    \n",
    "    # Extract newly generated tokens\n",
    "    gen_tokens = out[0, ids.shape[1]:]\n",
    "    raw_text = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
    "    # Save raw output with error handling\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    uid = ''.join(random.choices(string.ascii_uppercase + string.digits, k=6))\n",
    "    raw_file = raw_dir / f\"raw_{ts}_{uid}.txt\"\n",
    "    try:\n",
    "        raw_file.write_text(raw_text, encoding=\"utf-8\")\n",
    "        log(f\"Raw output written to {raw_file}\")\n",
    "    except Exception as e:\n",
    "        log(f\"ERROR writing raw output: {e}\")\n",
    "\n",
    "    # Extract all JSON array substrings (non-greedy)\n",
    "    matches = re.findall(r\"\\[.*?\\]\", raw_text, flags=re.DOTALL)\n",
    "    log(raw_text[:50])\n",
    "    if not matches:\n",
    "        log(f\"Failed to extract any JSON array from raw output, see {raw_file}\")\n",
    "        return raw_text\n",
    "    else:\n",
    "        data = []\n",
    "        for idx, json_str in enumerate(matches, start=1):\n",
    "            try:\n",
    "                obj = json.loads(json_str)\n",
    "                log(f\"Parsed JSON array #{idx} with {len(obj)} items.\")\n",
    "                data.extend(obj if isinstance(obj, list) else [obj])\n",
    "            except json.JSONDecodeError as e:\n",
    "                log(f\"ERROR parsing JSON array #{idx}: {e}\")\n",
    "    \n",
    "        # 3) write parsed data out to JSON\n",
    "        json_dir.mkdir(parents=True, exist_ok=True)\n",
    "        json_file = json_dir / f\"output_{ts}_{uid}.json\"\n",
    "        try:\n",
    "            # indent=2 for readability; you can omit if you want compact\n",
    "            json_file.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "            log(f\"Parsed JSON written to {json_file}\")\n",
    "        except Exception as e:\n",
    "            log(f\"ERROR writing parsed JSON: {e}\")\n",
    "       # return raw_text\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea54074b-d6a8-4e94-b4df-190ba36796f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-04 16:39:18] Generating 50000 entries in chunks of 12...\n",
      "[2025-06-04 16:39:18] Batch 1: generating 12 entries...\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Generate multi-language dev dataset.\")\n",
    "parser.add_argument(\"-n\", \"--num\",           type=int,   default=50000, help=\"Total entries to generate.\")\n",
    "parser.add_argument(\"--chunk_size\",          type=int,   default=12,    help=\"Entries per batch prompt.\")\n",
    "parser.add_argument(\"--temperature\",         type=float, default=1.0,   help=\"Sampling temperature.\")\n",
    "parser.add_argument(\"--max_new_tokens\",      type=int,   default=2400,  help=\"Max tokens per batch\")\n",
    "parser.add_argument(\"--do_sample\",           action=\"store_true\",    help=\"Enable sampling.\")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "# Update generation kwargs from CLI\n",
    "gen_kwargs.update({\n",
    "    \"max_new_tokens\": args.max_new_tokens,\n",
    "    \"do_sample\":      args.do_sample,\n",
    "    \"temperature\":    args.temperature if args.do_sample else 1.0,\n",
    "    \"top_p\":          0.95 if args.do_sample else 1.0,\n",
    "})\n",
    "\n",
    "total = args.num\n",
    "chunk = args.chunk_size\n",
    "all_data = []\n",
    "log(f\"Generating {total} entries in chunks of {chunk}...\")\n",
    "for start in range(0, total, chunk):\n",
    "    batch = min(chunk, total - start)\n",
    "    log(f\"Batch {start//chunk+1}: generating {batch} entries...\")\n",
    "    data = generate_dataset(batch)       # uses updated gen_kwargs\n",
    "    all_data.extend(data)\n",
    "\n",
    "\n",
    "# Save JSON dataset inside parsed_json/\n",
    "parsed_dir = Path(\"parsed_json\")\n",
    "parsed_dir.mkdir(exist_ok=True)\n",
    "out_json = parsed_dir / f\"multi_language_dev_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "try:\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        #json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
    "        try:\n",
    "            json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
    "        except TypeError as e:\n",
    "            log(f\"Serialization error: {e}\")\n",
    "    log(f\"Parsed JSON written to {out_json}\")\n",
    "except Exception as e:\n",
    "    log(f\"ERROR writing JSON: {e}\")\n",
    "    \n",
    "# Build DataFrame and save CSV\n",
    "try:\n",
    "    df = pd.DataFrame(all_data)\n",
    "    cols = [\"file_name\", \"file_path\", \"extension\", \"size_mb\", \"last_accessed_days\", \"category\" ,\"tool\", \"label\", \"reason\"]\n",
    "    df = df[[c for c in cols if c in df.columns]]\n",
    "    csv_file = out_json.with_suffix('.csv')\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    log(f\"DataFrame CSV written to {csv_file}\")\n",
    "except Exception as e:\n",
    "    log(f\"ERROR creating/saving DataFrame: {e}\")\n",
    "# Print preview\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a665e-ffd7-4456-a845-2db79855e74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51cc61-915a-4561-8e6c-01c3828a4912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f917c8c8-dec8-4b19-8e21-03ddeb61249e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff9454-4dee-45dc-82a9-e0a3c8a112c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc2f076-daee-4515-af6e-074c8fcc11ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a9e2bb-db75-4866-b353-e41ef6ed6454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c7f1ab-8bcc-4e59-a2f8-2abb165f2388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76750bc8-0611-4cf0-8b9e-b44ca53fd974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
